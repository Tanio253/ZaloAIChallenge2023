{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6OhynkMUf4g"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF7adqPCJ0jA"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    GPTQConfig,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    AutoPeftModelForCausalLM\n",
        ")\n",
        "import json\n",
        "import copy\n",
        "from trl import SFTTrainer\n",
        "import wandb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPA2R16w9ERj"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "wandb_project = 'ToraFT'\n",
        "if len(wandb_project)>0:\n",
        "  os.environ['WANDB_PROJECT'] = wandb_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KSdcjGsQzNG"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gLxulX9QTZi"
      },
      "source": [
        "PREPARE CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V09OQYHLU4Vr"
      },
      "source": [
        "Prompt: System_prompt Câu hỏi: Q1 Đáp án: A1 Giải thích: E1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBlU2qK4NrEy"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "  MODEL_ID = 'llm-agents/tora-13b-v1.0'\n",
        "  REVISION = 'main'\n",
        "  OUTPUT_DIR = 'ToraZaloFT'\n",
        "  PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
        "  GRADIENT_ACCUMULATION_STEPS = 32\n",
        "  OPTIM = 'paged_adamw_32bit' #8or32\n",
        "  LEARNING_RATE = 2e-4\n",
        "  LR_SCHEDULER_TYPE = 'constant'\n",
        "  LOGGING_STEPS = 32\n",
        "  SAVE_STRATEGY = 'steps'\n",
        "  SAVE_STEPS = 32\n",
        "  WARMUP_STEPS = 5\n",
        "  EVAL_STEPS = 32\n",
        "  LOGGING_DIR = './logs'\n",
        "  MAX_STEPS = 160\n",
        "  NUM_TRAIN_EPOCHS = 2\n",
        "  FP16 = True\n",
        "  PUSH_TO_HUB = False\n",
        "  DATASET_TEXT_FIELD = 'content'\n",
        "  MAX_SEQ_LENGTH = 4096\n",
        "  REPORT_TO = 'wandb'\n",
        "  PACKING = False\n",
        "  DO_EVAL = True\n",
        "  NEFTUNE_NOISE_ALPHA = 5\n",
        "  EVALUATION_STRATEGY = 'steps'\n",
        "  R = 128\n",
        "  LORA_ALPHA = 256\n",
        "  LORA_DROPOUT = 0.05\n",
        "  TARGET_MODULES = ['q_proj', 'v_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'up_proj', 'lm_head']\n",
        "  BIAS = 'none'\n",
        "  TASK_TYPE = 'CAUSAL_LM'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyoXZ_6Ke6o"
      },
      "source": [
        "\n",
        "\n",
        "PREPARE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8nd98RrgYG9"
      },
      "outputs": [],
      "source": [
        "train_file_path = './data/ztrain/mTrain.json'\n",
        "val_file_path = './data/ztrain/mVal.json'\n",
        "countA = 0\n",
        "countB = 0\n",
        "countC = 0\n",
        "countD = 0\n",
        "with open(train_file_path, 'r') as file:\n",
        "    train_data = json.load(file)\n",
        "with open(val_file_path, 'r') as file:\n",
        "    val_data = json.load(file)\n",
        "train_data = train_data['data']\n",
        "len_train_data = len(train_data)\n",
        "print(f\"Length train dataset: {len_train_data}\")\n",
        "val_data = val_data['data']\n",
        "len_val_data = len(val_data)\n",
        "print(f\"Length validation dataset: {len_val_data}\")\n",
        "zalo_train_data = {'question': [],\n",
        "                   'choices': [],\n",
        "                   'answer': [],\n",
        "                   'explanation': [],\n",
        "                   'id': []}\n",
        "zalo_val_data = {'question': [],\n",
        "                   'choices': [],\n",
        "                   'answer': [],\n",
        "                   'explanation': [],\n",
        "                   'id': []}\n",
        "for i in range(len_train_data):\n",
        "    zalo_train_data['question'].append(train_data[i]['question'])\n",
        "    zalo_train_data['choices'].append(train_data[i]['choices'])\n",
        "    zalo_train_data['answer'].append(train_data[i]['answer'])\n",
        "    zalo_train_data['id'].append(train_data[i]['id'])\n",
        "    zalo_train_data['explanation'].append(train_data[i]['explanation'])\n",
        "    if 'A.' in train_data[i]['answer']:\n",
        "      countA+=1\n",
        "    elif 'B.' in train_data[i]['answer']:\n",
        "      countB+=1\n",
        "    elif 'C.' in train_data[i]['answer']:\n",
        "      countC+=1\n",
        "    elif 'D.' in train_data[i]['answer']:\n",
        "      countD+=1\n",
        "print(f\"Training Data:\\nA: {countA} B: {countB} C: {countC} D: {countD}\")\n",
        "countA = 0\n",
        "countB = 0\n",
        "countC = 0\n",
        "countD = 0\n",
        "for i in range(len_val_data):\n",
        "    zalo_val_data['question'].append(val_data[i]['question'])\n",
        "    zalo_val_data['choices'].append(val_data[i]['choices'])\n",
        "    zalo_val_data['answer'].append(val_data[i]['answer'])\n",
        "    zalo_val_data['id'].append(val_data[i]['id'])\n",
        "    zalo_val_data['explanation'].append(val_data[i]['explanation'])\n",
        "    if 'A.' in val_data[i]['answer']:\n",
        "      countA+=1\n",
        "    elif 'B.' in val_data[i]['answer']:\n",
        "      countB+=1\n",
        "    elif 'C.' in val_data[i]['answer']:\n",
        "      countC+=1\n",
        "    elif 'D.' in val_data[i]['answer']:\n",
        "      countD+=1\n",
        "print(f\"Validation Data:\\nA: {countA} B: {countB} C: {countC} D: {countD}\")\n",
        "# Now 'data' contains the content of the JSON file as a dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvh_R9vbKdBa"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\n",
        "# \"\"\"Bạn là một người hỗ trợ giúp tôi giải những bài toán sau đây. Sẽ có 4 đáp án A, B, C, D. \\\n",
        "# Hãy hít một hơi thật sâu, sau đó từng bước một giải ra và chọn 1 trong 4 đáp án A, B, C, D.\"\"\"\n",
        "def preprocess(samples):\n",
        "  # conv_prefix = f\"{system_prompt}\"\n",
        "  # batch = []\n",
        "  # print(samples['question'])\n",
        "  # len_dataset = len(samples['question'])\n",
        "  # print(5*'-'+ f\"length of dataset is:{len_dataset}\" + 5*'-')\n",
        "  # for i in range(len_dataset):\n",
        "  question =  f\"<|user|>\\n{samples['question']}\\n\"\n",
        "  choices_list = samples['choices'].copy()\n",
        "  choices = \" \".join(choices_list)\n",
        "  question = question + choices + '\\n'\n",
        "  explanation = f\"{samples['explanation']}\"\n",
        "  answer = f\"<|assistant|>\\n{explanation}\\nAnswer:{samples['answer']}\"\n",
        "  formatted_conv = f\"{question}{answer}\"\n",
        "  return {'content':formatted_conv}\n",
        "train_data = Dataset.from_dict(zalo_train_data, split = 'train')\n",
        "train_data = train_data.map(\n",
        "    preprocess,\n",
        "    # batched = True,\n",
        "    remove_columns = train_data.column_names\n",
        ")\n",
        "train_data = train_data.shuffle(100)\n",
        "print(train_data[2])\n",
        "\n",
        "val_data = Dataset.from_dict(zalo_val_data, split = 'val')\n",
        "val_data = val_data.map(\n",
        "    preprocess,\n",
        "    # batched = True,\n",
        "    remove_columns = val_data.column_names\n",
        ")\n",
        "val_data = val_data.shuffle(100)\n",
        "print(val_data[0])\n",
        "# print(count1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDotCtpqvM91"
      },
      "source": [
        "PREPARE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6o3w8gP4gOF"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_ID, trust_remote_code = True, revision = Config.REVISION)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD2dKxeDvMvI"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(Config.MODEL_ID,\n",
        "                                            #  quantization_config = GPTQConfig(bits = 4, use_exllama= True),\n",
        "                                             device_map = 'auto',\n",
        "                                             revision = Config.REVISION,\n",
        "                                             trust_remote_code = True,\n",
        "                                            #  use_flash_attention_2 = True\n",
        "                                             )\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F60Enp5tuz0l"
      },
      "source": [
        "PREPARE PEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJweDw-buyfe"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(r = Config.R,\n",
        "                    lora_alpha = Config.LORA_ALPHA,\n",
        "                    lora_dropout = Config.LORA_DROPOUT,\n",
        "                    target_modules = Config.TARGET_MODULES,\n",
        "                    bias = Config.BIAS,\n",
        "                    task_type = Config.TASK_TYPE)\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WhOILhiyHqv"
      },
      "source": [
        "PREPARE TRAINING ARGUMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR9LM2gguXjf"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir = Config.OUTPUT_DIR,\n",
        "    per_device_train_batch_size = Config.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "    gradient_accumulation_steps = Config.GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim = Config.OPTIM,\n",
        "    learning_rate = Config.LEARNING_RATE,\n",
        "    save_strategy = Config.SAVE_STRATEGY,\n",
        "    lr_scheduler_type = Config.LR_SCHEDULER_TYPE,\n",
        "    eval_steps = Config.EVAL_STEPS,\n",
        "    logging_steps = Config.LOGGING_STEPS,\n",
        "    max_steps = Config.MAX_STEPS,\n",
        "    fp16 = Config.FP16,\n",
        "    save_steps = Config.SAVE_STEPS,\n",
        "    logging_dir = Config.LOGGING_DIR,\n",
        "    report_to = Config.REPORT_TO,\n",
        "    do_eval = Config.DO_EVAL,\n",
        "    warmup_steps = Config.WARMUP_STEPS,\n",
        "    push_to_hub = Config.PUSH_TO_HUB,\n",
        "    neftune_noise_alpha = Config.NEFTUNE_NOISE_ALPHA,\n",
        "    evaluation_strategy = Config.EVALUATION_STRATEGY,\n",
        "    num_train_epochs = Config.NUM_TRAIN_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38FRZfa3yKKW"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(model = model,\n",
        "                     args = training_arguments,\n",
        "                     train_dataset = train_data,\n",
        "                     eval_dataset = val_data,\n",
        "                     peft_config = peft_config,\n",
        "                     tokenizer = tokenizer,\n",
        "                     packing = False,\n",
        "                     dataset_text_field = 'content',\n",
        "                     max_seq_length = Config.MAX_SEQ_LENGTH)\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
